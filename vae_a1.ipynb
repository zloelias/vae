{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vae_a1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDSkHabflm85"
      },
      "source": [
        "#### List of problems:\n",
        "[Task 1](#Task1) 20 points\n",
        "\n",
        "[Task 2](#Task2) 15 points\n",
        "\n",
        "[Task 3](#Task3) 5 points \n",
        "\n",
        "[Task 4](#Task4) 10 points \n",
        "\n",
        "------ Total : 50 points  -------\n",
        "\n",
        "[Bonus](#Bonus) 10 point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3KowYd-pQGL"
      },
      "source": [
        "## Variational Autoencoders\n",
        "\n",
        "VAEs consider two-step generative process by a prior over latent space $p(z)$ and a conditional generative distribution $p_{\\theta}(x|z)$, which is parametrized by a deep neural network (DNN). Our goal is to maximize marginal log-likehood  which is intractable in general case. Therefore, variational inference (VI) framework is considered.\n",
        "\n",
        "\\begin{equation*}\n",
        "    \\begin{aligned}\n",
        "    & \\log p(x) \\geq \\mathcal{L}(x;\\theta;q) = \\mathbb{E}_{z\\sim q(z)}[\\log p_{\\theta}(x|z)] - \\text{KL}[q(z)\\|p(z)],\n",
        "    \\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "where $q(z|x)$ is a variational posterior distribution. Given data distribution $p_e(x) = \\frac1N\\sum_{i=1}^N \\delta_{x_i}(x)$ we aim at maximizing the average marginal log-likelihood. Following the variational auto-encoder architecture amortized inference is proposed by choice of the variational distribution $q_{\\phi}(z|x)$ which is also parametrized by DNN.\n",
        "\n",
        "\\begin{equation*}\n",
        "    \\begin{aligned}\n",
        "    & \\arg\\max\\limits_{\\phi, \\theta}\\mathbb{E}_{x}\\mathcal{L}(x,\\phi,\\theta)=\\arg\\max\\limits_{\\phi, \\theta}\\mathbb{E}_{x}\\mathbb{E}_{z\\sim q(z)}[\\log p_{\\theta}(x|z)] - \\mathbb{E}_x \\text{KL}[q_{\\phi}(z|x)\\|p(z)].\n",
        "    \\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "To evaluate the performance of the VAE approach, we will estimate a negative log likelihood (NLL) on the test set. NLL is calculated by importance sampling method:\n",
        "\\begin{equation*}\n",
        "   - \\log p(x) \\approx - \\log \\frac{1}{K} \n",
        "   \\sum_{i = 1}^K \\frac{p_\\theta(x | z_i) p(z_i)}{q_\\phi(z_i | x)},\\,\\,\\,\\,z_i \\sim q_\\phi(z | x).     \n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfjN-b-Ylm86"
      },
      "source": [
        "### References\n",
        "1. Auto-Encoding Variational Bayes https://arxiv.org/pdf/1312.6114.pdf\n",
        "2. Beta-VAE https://pdfs.semanticscholar.org/a902/26c41b79f8b06007609f39f82757073641e2.pdf\n",
        "3. Importance Weighted Autoencoders https://arxiv.org/pdf/1509.00519.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6sPNPalm86"
      },
      "source": [
        "## VAE\n",
        "Below you can find empty class for VAE model. It contians methods, which will help you to make all the tasks in this aasignment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "der7Ck9SbTa3",
        "outputId": "f4e3d26b-fbc2-46bf-99ed-b543ea02f87f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wovzCNIRXFbT"
      },
      "source": [
        "import math \n",
        "def log_gaussian(x, mean, logvar, dim=None):\n",
        "    log_normal = -0.5 * (math.log(2.0*math.pi) + logvar + \n",
        "                         torch.pow(x - mean, 2) / (logvar.exp()+1e-5))\n",
        "    return log_normal.sum(dim)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVers8V3lm87"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "encode_block = lambda i, o: nn.Sequential(\n",
        "    nn.Conv2d(i, o, kernel_size=5, padding=0),\n",
        "    nn.ELU(),\n",
        "    nn.MaxPool2d(5)\n",
        ")\n",
        "\n",
        "decoder_block = lambda i, o, k, s, p, a, op: nn.Sequential(\n",
        "    nn.ConvTranspose2d(i, o, k, s, p, output_padding=op),\n",
        "    nn.ELU() if a else nn.Identity()\n",
        ")\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        \"\"\"\n",
        "        z_dim: int, dimention of the latent space\n",
        "        x_dim: int, input image will have size (3, x_dim, x_dim)\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        # Initialize Encoder and Decoder networks\n",
        "        \n",
        "        self.encoder = nn.Sequential(OrderedDict([\n",
        "            ('encode1', encode_block(3, 16)),    # N x 16 x 12 x 12\n",
        "            ('flatten', nn.Flatten(start_dim=1)),\n",
        "            ('elr', nn.Linear(16 * 12 * 12, self.hid_dim)),\n",
        "        ]))\n",
        "\n",
        "        self.mu = nn.Linear(self.hid_dim, self.hid_dim)\n",
        "        self.logvar = nn.Linear(self.hid_dim, self.hid_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(OrderedDict([\n",
        "            ('dlr', nn.Linear(self.hid_dim, 16 * 12 * 12)),\n",
        "            ('unflatten', nn.Unflatten(1, (16, 12, 12))),\n",
        "            ('upsample', nn.Upsample(size=(64, 64))),\n",
        "            ('decode3', decoder_block(16, 3, 1, 1, 0, True, 0)),\n",
        "        ]))\n",
        "       \n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "        \n",
        "    def q_z(self, x):\n",
        "        \"\"\"\n",
        "        VARIATIONAL POSTERIOR\n",
        "        :param x: input image\n",
        "        :return: parameters of q(z|x), (MB, hid_dim)\n",
        "        \"\"\"\n",
        "        ## YOUR CODE HERE\n",
        "        z = self.encoder(x)\n",
        "        z_mean = self.mu(z)\n",
        "        z_logvar = self.logvar(z)\n",
        "        return Normal(z_mean, z_logvar)\n",
        "\n",
        "    def p_x(self, z):\n",
        "        \"\"\"\n",
        "        GENERATIVE DISTRIBUTION\n",
        "        :param z: latent vector          (MB, hid_dim)\n",
        "        :return: parameters of p(x|z)    (MB, inp_dim)\n",
        "        \"\"\"\n",
        "        ## YOUR CODE HERE\n",
        "        x = self.decoder(z)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Encoder the image, sample z and decode \n",
        "        :param x: input image\n",
        "        :return: parameters of p(x|z_hat), z_hat, parameters of q(z|x)\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        x_encoded = self.encoder(x)\n",
        "        z_mu, z_logvar = self.mu(x_encoded), self.logvar(x_encoded)\n",
        "        z_sample = self.reparametrize(z_mu, z_logvar)\n",
        "        x_mu = self.decoder(z_mu)\n",
        "        x_logvar = self.decoder(z_logvar)\n",
        "        return x_mu, x_logvar, z_sample, z_mu, z_logvar\n",
        "\n",
        "    def log_p_z(self, z):\n",
        "        \"\"\"\n",
        "        Log density of the Prior\n",
        "        :param z: latent vector     (MB, hid_dim)\n",
        "        :return: \\sum_i log p(z_i)  (1, )\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        return torch.log(z).sum()\n",
        "\n",
        "    def reconstruct_x(self, x):\n",
        "        x_mean, _, _, _, _ = self.forward(x)\n",
        "        return x_mean\n",
        "\n",
        "    def kl(self, z, z_mean, z_logvar):\n",
        "        \"\"\"\n",
        "        KL-divergence between p(z) and q(z|x)\n",
        "        :param z:                               (MB, hid_dim)\n",
        "        :param z_mean: mean of q(z|x)           (MB, hid_dim)\n",
        "        :param z_logvar: log variance of q(z|x) (MB, hid_dim)\n",
        "        :return: KL                             (MB, )\n",
        "        \"\"\"\n",
        "        # YOUR CODE \n",
        "        '''\n",
        "        x_recon = self.decoder\n",
        "        p = Normal(torch.zeros_like(z_mean), torch.ones_like(z_logvar))\n",
        "        q = Normal(z_mean, z_logvar.mul(0.5).exp_())\n",
        "        log_qzx = q.log_prob(z)\n",
        "        log_pz = p.log_prob(z)\n",
        "        kl = (log_qzx - log_pz)\n",
        "        kl = kl.sum(-1)\n",
        "        return kl\n",
        "        '''\n",
        "        return -0.5 * torch.mean(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
        "\n",
        "    def calculate_loss(self, x, beta=1.):\n",
        "        \"\"\"\n",
        "        Given the input batch, compute the negative ELBO \n",
        "        :param x:   (MB, inp_dim)\n",
        "        :param beta: Float\n",
        "        :return: nll + beta * KL  (MB, ) or (1, )\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def calculate_nll(self, X, K=100):\n",
        "        \"\"\"\n",
        "        Estimate NLL by importance sampling \n",
        "        (see VAE seminar, but be carefull with dimetions)\n",
        "        :param X: dataset, (N, 3, w, h)\n",
        "        :param samples: Samples per observation\n",
        "        :return: IS estimate\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE       \n",
        "        X_encoded = self.encoder(X)\n",
        "        Z_mean, Z_logvar = self.mu(X_encoded), self.logvar(X_encoded)\n",
        "        p = Normal(torch.zeros_like(z_mean), torch.ones_like(z_logvar))\n",
        "        q = Normal(z_mean, z_logvar.mul(0.5).exp_())\n",
        "        log_qzx = q.log_prob(z)\n",
        "        log_pz = p.log_prob(z)\n",
        "\n",
        "    def generate_x(self, N=25):\n",
        "        \"\"\"\n",
        "        Sample, using you VAE: sample z from prior and decode it \n",
        "        :param N: number of samples\n",
        "        :return: X (N, inp_size)\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    @staticmethod\n",
        "    def reparametrize(mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = torch.FloatTensor(std.size()).normal_().to(mu.device)\n",
        "        return eps.mul(std).add_(mu)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6kHJyaLlm8_"
      },
      "source": [
        "## Generalization\n",
        "\n",
        "The size of the Dataset, that is used to train generative model is often exponentially small compared to the support of density $p(x)$. Thus, it is important to be able to evaluate genralization abilities of the learned model.\n",
        "\n",
        "In this assignment you will be asked to evaluate generalization ability of the VAE, using the approach discussed in https://arxiv.org/abs/1811.03259. \n",
        "\n",
        "Authors propose to study the generalization ability of the generative model, using **probing features** - functions which map image to a value. E.g., number of objects on the images.\n",
        "\n",
        "\n",
        "We will use the dataset with dots, which can be downloaded [here](https://drive.google.com/open?id=1CsDMOIGEsD1l3BLhuQDfEfEmLEb83wMz). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUnt_E1ulm8_"
      },
      "source": [
        "---\n",
        "<a id='Task1'></a>\n",
        "**Task 1. [20 pts]: Training**\n",
        "* Train your VAE on the **subset** of images contating only 3 dots (use batch 0-5 for training and leave 6 and 7 for validation). \n",
        "* Plot ELBO vs Iteration, KL vs Iteration and NLL vs Iteration during training.\n",
        "* Plot 10 pairs of `image`-`reconstruction` for 10 random images from the validation dataset \n",
        "* Plot 10 samples from the model\n",
        "\n",
        "Note, that the task is considered completed only if your model produces reasonable **reconstructions** and **samples**. By resonable I mean:\n",
        "- Reconstructions and true images have the same number of dots of similar colors\n",
        "- Samples have dots of different colors on them (they may not have perfect shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e39uPiBhpQGP"
      },
      "source": [
        "---\n",
        "Some hints, that might be usefull (you do not have to use all of them):\n",
        "- Use CNNs for encoder and decoder\n",
        "- Use gaussian distribution for $p(x|z)$\n",
        "- **Fix** variance of the $p(x|z)$ and train only mean value (aka NLL is MSE loss)\n",
        "- Scale pixels of the input (dataset) and output (generated) images to [-1,1] range\n",
        "- Use `Upsample` + `Conv` instead of `ConvTranspose` in the decoder\n",
        "- Use $\\beta$-VAE objective instead of simple ELBO:\n",
        "    $$ -NLL + \\beta \\text{KL}(q(z|x)\\|p(z))$$\n",
        "$\\beta$ is a hyperparameter. You can either fix it, or use so-called $\\beta$-annealing. In the latter case, we gradually increase the value of $\\beta$ from 0 during training.\n",
        "- If reconstructions look nice but samples are bad, you probably need to put more weight on the KL-term  (larger $\\beta$)\n",
        "\n",
        "+ other standard DL tricks, e.g. lr annealing, early stopping, augmentations, etc.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvjMouY4pQGQ"
      },
      "source": [
        "# Load the data, define train and validation datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class DotsDataset(Dataset):\n",
        "    def __init__(self, data_dir, file_numbers, transform=None):\n",
        "        self.images = torch.zeros(0, 3, 64, 64)\n",
        "        for file_number in file_numbers:\n",
        "            batch = np.load(f'{data_dir}batch{file_number}.npz')\n",
        "            batch = torch.tensor(batch['images']).permute(0, 3, 1, 2)\n",
        "            self.images = torch.cat((self.images, batch))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.images[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNz-4PNKsgHF"
      },
      "source": [
        "data_dir = '/content/gdrive/My Drive/Colab Notebooks/pr3/vae model and generalization/dots/3_dots/'\n",
        "\n",
        "train_dataset = DotsDataset(data_dir, list(range(6)))\n",
        "val_dataset = DotsDataset(data_dir, list(range(6, 8)))\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=2, pin_memory=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                             num_workers=2, pin_memory=True)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "0m49OyEO19bH",
        "outputId": "05e08589-d49f-4599-b95b-c475f0db7459"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "plt.imshow(train_dataset[random.randrange(len(train_dataset))].permute(1, 2, 0))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f353ca52050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARQElEQVR4nO3dfZAcdZ3H8feH3TwZlRCyxJAEkysDiKUEagloKIQEPOA48e4CCpSXsnLmHxTw9BS0ysI7r0rLKxWrLKtywpnz5FHFxJyAEKEAuYJsJEBIgIS4QEKS3QDhSQQTvvfH9HZmtnbZzsz0zJLf51W1td9+mOlvmPlM/3q26VZEYGYHvoPa3YCZtYbDbpYIh90sEQ67WSIcdrNEOOxmiWgo7JLOlPS4pM2SLm9WU2bWfKr37+ySOoAngDOArcAa4IKI2NC89sysWTobeOw8YHNEbAGQdD1wLjBs2KdMmRKzZs1qYJNm9lZ6e3vZtWuXhlrWSNinA89UTW8FTnyrB8yaNYuenp4GNmlmb6W7u3vYZaV/QSdpqaQeST39/f1lb87MhtFI2LcBM6umZ2TzakTEsojojojurq6uBjZnZo1oJOxrgDmSZksaC3wKWNmctsys2eo+Zo+IPZI+B9wGdADXRMSjTevMzJqqkS/oiIjfAL9pUi9mViKfQWeWCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WiBHDLukaSX2S1lfNmyzpdkmbst+HlNummTWqyJ79J8CZg+ZdDqyOiDnA6mzazEaxEcMeEXcDzw+afS6wPKuXA59ocl9m1mT1HrNPjYjtWb0DmNqkfsysJA1/QRcRAcRwyyUtldQjqae/v7/RzZlZneq9ZfNOSdMiYrukaUDfcCtGxDJgGUB3d/ewHwpmo0Vl/7VP78a78/q5HZtqlnV2jsvr2R84Na8PPnRmOc01oN49+0pgcVYvBlY0px0zK0uRP71dB/wfcJSkrZKWAN8CzpC0CTg9mzazUWzEYXxEXDDMooVN7sXMSlTvMbvZAeXJR+7I63tWfqdm2fbeBws9x9jxE/P6qOPPqVm28Lx/zesJ72zPOWg+XdYsEQ67WSI8jLckbXn0dzXTv1q2NK/f+PMrdT3nG39+Na8fue+GmmUv7tqa1+dd8j81y8aOm0greM9ulgiH3SwRDrtZInzMbsmoPg128J/X6j1OL+rpJ36f1w/fe23Nsu6Fny112wO8ZzdLhMNulggP4y0ZTz22byj97Ja1betjw5pf1Ux7GG9mTeWwmyXCw3hLxvM7N7e7BQB29/W2Zbves5slwmE3S4TDbpYIH7NbMjrHjBt5pRboaFMf3rObJcJhN0uEh/GWjNkfOC2vx004uGbZ66+92LI+3nv0yS3bVjXv2c0S4bCbJcJhN0uEj9lt1NrwUm9e//HV7Xk9/qAxNevNP/SD+5Z1Dv9nrXdNek9ev7/73Jpl6+7573rbLKSjqq9j519Y6raGU+T2TzMl3Slpg6RHJV2azZ8s6XZJm7Lf7bnyvZkVUmQYvwf4YkQcA5wEXCzpGOByYHVEzAFWZ9NmNkoVudfbdmB7Vr8saSMwHTgXODVbbTlwF/CVUrq0A9aa5zfm9Q+f/GXNsvt2PZLXe3lz2Oc4fPyUvP67w0+pWXbJnPPyuvOgjrxecN7Xa9Z78bmn8/qPG+4aoeuRdQw6nDj9/H23f5p55EkNP3899usLOkmzgOOA+4Gp2QcBwA5galM7M7OmKhx2Se8EfgFcFhEvVS+LymU7Y5jHLZXUI6mnv7+/oWbNrH6Fwi5pDJWg/ywiBsZaOyVNy5ZPA/qGemxELIuI7ojo7urqakbPZlaHEY/ZJQm4GtgYEd+tWrQSWAx8K/u9opQO7YBy33OP1Ex/7g/73lIv7nl18OqFPPvnXXn9wy21x/1Pv7Yzr7977OfzetyEd9est+hzP83rwfdp27Dm5rx+fueWmmWdnWPz+oij5+f1sR+5oGa9GXNOHP4f0CJF/s4+H/g08Iikddm8r1IJ+Y2SlgBPAeeX06KZNUORb+PvBTTM4oXNbcfMyuIz6Kx01bdd+s7j19Usq3foXtSvt++7Vvzph52Q1+cc/pGa9aovbHHcR/+xZtng6bcrnxtvlgiH3SwRHsZb6e7d9XBeP/xi+67d/uvt9+T14GF8CrxnN0uEw26WCIfdLBE+ZrfSPfHKM+1uAYBNr2xrdwtt5T27WSIcdrNEeBhvpetUx8grtUCn0t63pf2vN0uIw26WCIfdLBE+ZrfSnTT5A3k9htrj97+wt2V9HD/p6JZtazTynt0sEQ67WSI8jLfSHfXuI/L6o4cdV7Psjr6eUrfdWbU/+/sZHy11W6Od9+xmiXDYzRLhYby11Nff/5ma6a2v7btxyGMvP9Xw83cO2n/9y1EX5fW8ye9v+PnfzrxnN0uEw26WCIfdLBE+ZreWmv6O2vv9LT/ha3n906duq1n2vzvuy+utr+67leC4jrE1680/9IN5ff7MBTXLTj3s+PqbPcCMuGeXNF7SA5IekvSopG9k82dLul/SZkk3SBo70nOZWfsUGca/DiyIiGOBucCZkk4Cvg18LyLeB7wALCmvTTNrVJF7vQXwSjY5JvsJYAFwYTZ/OXAl8KPmt2gHsinjJuX1F478ZM2yy+bsu1fon/a+ntdjD6p92445yEejRRS9P3tHdgfXPuB24Elgd0TsyVbZCkwvp0Uza4ZCYY+IvRExF5gBzAMK/7+CkpZK6pHU09/fP/IDzKwU+/Wnt4jYDdwJfBiYJGlg/DQDGPI6vRGxLCK6I6K7q6trqFXMrAWKfBvfJWlSVk8AzgA2Ugn9omy1xcCKspq0NEnKfyZ2js9/xhzUWfNjxRT5LzUNWC6pg8qHw40RsUrSBuB6Sd8EHgSuLrFPM2tQkW/jHwaOG2L+FirH72b2NuAxUBPs3bOnZrrnllv21b+5pWbZc88+m9fjJkyoWXbM/Pl5PX/RP+T15GnTmtKnpc3nxpslwmE3S4SH8XV67eWX83rZZV+oWbb+7rvres7Na9fm9V3XXpvXS/7jOzXrVQ/3zYrynt0sEQ67WSIcdrNE+Ji9Ttf92zfzut5j9Lfy0q5deX31l75Us+yrN92U14fOmNH0bduByXt2s0Q47GaJ8DB+P/Q9/XRer7311pZt96Vdz9VM//4Xv8zrj196Scv6sLc379nNEuGwmyXCYTdLhI/Z98O6O1bn9Ruvvda2Ptbfc09e+5jdivKe3SwRDrtZIjyM3w9v/OlP7W4BgNdHSR/29uI9u1kiHHazRHgYvx8OmfaedrcAwCG+Jp3VwXt2s0Q47GaJcNjNEuFj9v3QffbZeb3yqh/k9fPbt7e0jxPOOqul27MDQ+E9e3bb5gclrcqmZ0u6X9JmSTdIGltem2bWqP0Zxl9K5YaOA74NfC8i3ge8ACxpZmNm1lyFhvGSZgB/A/w78M+SBCwALsxWWQ5cCfyohB5HjerbNf31Z/8pr6uvRwdARFO3e+S82lvqzfvbc5r6/JaGonv27wNfBt7Mpg8FdkfEwE3OtgLTm9ybmTVRkfuznwP0RcTakdYd5vFLJfVI6unv76/nKcysCYrs2ecDH5fUC1xPZfh+FTBJ0sBhwAxg21APjohlEdEdEd1dXV1NaNnM6lHk/uxXAFcASDoV+FJEXCTpJmARlQ+AxcCKEvscdRZ8+tN5HW++WbPs1v/8cV7v3rmz8HN2jBmT13NPPz2vL7ryypr1xowbV/g5zQY0clLNV6h8WbeZyjH81c1pyczKsF8n1UTEXcBdWb0FmPdW65vZ6OEz6Jpg4eLFNdPzFy3K6wdWrapZVn223bjxE2qWfei00/J6+lFHNrNFM58bb5YKh90sER7Gl2D8xIl5fconP9nGTsz28Z7dLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0SUfT+7L3Ay8BeYE9EdEuaDNwAzAJ6gfMj4oVy2jSzRu3Pnv20iJgbEd3Z9OXA6oiYA6zOps1slGpkGH8usDyrlwOfaLwdMytL0bAH8FtJayUtzeZNjYiBG5ftAKY2vTsza5qid4Q5OSK2SToMuF3SY9ULIyIkxVAPzD4clgIcccQRDTVrZvUrtGePiG3Z7z7gZiq3at4paRpA9rtvmMcui4juiOju6upqTtdmtt9GDLukiZLeNVADHwPWAyuBgXsVLwZWlNWkmTWuyDB+KnCzpIH1r42IWyWtAW6UtAR4Cji/vDbNrFEjhj0itgDHDjH/OWBhGU2ZWfP5DDqzRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRBQKu6RJkn4u6TFJGyV9WNJkSbdL2pT9PqTsZs2sfkX37FcBt0bE0VRuBbURuBxYHRFzgNXZtJmNUkXu4nowcApwNUBEvBERu4FzgeXZasuBT5TVpJk1rsiefTbQD/yXpAcl/Ti7dfPUiNierbODyt1ezWyUKhL2TuB44EcRcRzwKoOG7BERQAz1YElLJfVI6unv72+0XzOrU5GwbwW2RsT92fTPqYR/p6RpANnvvqEeHBHLIqI7Irq7urqa0bOZ1WHEsEfEDuAZSUdlsxYCG4CVwOJs3mJgRSkdmllTdBZc7/PAzySNBbYAn6HyQXGjpCXAU8D55bRoZs1QKOwRsQ7oHmLRwua2Y2Zl8Rl0Zolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiVDmtvUUbk/qpnIAzBdjVsg0PbTT0AO5jMPdRa3/7eG9EDHleekvDnm9U6omIoU7SSaoH9+E+WtmHh/FmiXDYzRLRrrAva9N2q42GHsB9DOY+ajWtj7Ycs5tZ63kYb5aIloZd0pmSHpe0WVLLrkYr6RpJfZLWV81r+aWwJc2UdKekDZIelXRpO3qRNF7SA5Ieyvr4RjZ/tqT7s9fnhuz6BaWT1JFd33BVu/qQ1CvpEUnrJPVk89rxHintsu0tC7ukDuCHwFnAMcAFko5p0eZ/Apw5aF47LoW9B/hiRBwDnARcnP03aHUvrwMLIuJYYC5wpqSTgG8D34uI9wEvAEtK7mPApVQuTz6gXX2cFhFzq/7U1Y73SHmXbY+IlvwAHwZuq5q+AriihdufBayvmn4cmJbV04DHW9VLVQ8rgDPa2QvwDuAPwIlUTt7oHOr1KnH7M7I38AJgFaA29dELTBk0r6WvC3Aw8Eey79Ka3Ucrh/HTgWeqprdm89qlrZfCljQLOA64vx29ZEPndVQuFHo78CSwOyL2ZKu06vX5PvBl4M1s+tA29RHAbyWtlbQ0m9fq16XUy7b7Czre+lLYZZD0TuAXwGUR8VI7eomIvRExl8qedR5wdNnbHEzSOUBfRKxt9baHcHJEHE/lMPNiSadUL2zR69LQZdtH0sqwbwNmVk3PyOa1S6FLYTebpDFUgv6ziPhlO3sBiMrdfe6kMlyeJGnguoSteH3mAx+X1AtcT2Uof1Ub+iAitmW/+4CbqXwAtvp1aeiy7SNpZdjXAHOyb1rHAp+icjnqdmn5pbAlicpttDZGxHfb1YukLkmTsnoCle8NNlIJ/aJW9RERV0TEjIiYReX98LuIuKjVfUiaKOldAzXwMWA9LX5douzLtpf9xcegLxrOBp6gcnz4tRZu9zpgO/AXKp+eS6gcG64GNgF3AJNb0MfJVIZgDwPrsp+zW90L8CHgwayP9cDXs/l/BTwAbAZuAsa18DU6FVjVjj6y7T2U/Tw68N5s03tkLtCTvTa/Ag5pVh8+g84sEf6CziwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNuloj/B8hiQOFNG1v9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOi2HQ3qlm9A"
      },
      "source": [
        "# Training VAE (do not forget to save checkpoint, \n",
        "# and send it along with HW solution)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def eval_elbo(X, vae):\n",
        "  \"\"\"\n",
        "  Compute reconstruction loss and KL term for a given mini-batch\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # encode X\n",
        "  z = vae.encoder(X)\n",
        "  z_mu, z_logvar = vae.mu(z), vae.logvar(z)\n",
        "  \n",
        "  # compute KL\n",
        "  kl = vae.kl(z, z_mu, z_logvar)\n",
        "\n",
        "  # sample z from var posterior\n",
        "  z_sample = vae.reparametrize(z_mu, z_logvar)\n",
        "  \n",
        "  # decode z\n",
        "  X_recon = vae.decoder(z_sample)\n",
        "  rec_loss = F.binary_cross_entropy(X_recon, X, size_average=False)\n",
        "  \n",
        "  return rec_loss.mean(), kl.mean()\n",
        "\n",
        "\n",
        "def train_vae(train_loader, max_epoch, vae):\n",
        "  kl_hist = []\n",
        "  rec_hist = []\n",
        "  all_params = vae.parameters()\n",
        "  optim = torch.optim.Adam(all_params, lr=1e-4)\n",
        "  for e in range(max_epoch):\n",
        "    tot_rec = 0.\n",
        "    tot_kl = 0.\n",
        "    for X in train_loader:\n",
        "      optim.zero_grad()\n",
        "      X = X.to(device)\n",
        "      rec_loss, kl = eval_elbo(X, vae)\n",
        "      loss = rec_loss + kl #* alpha\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "      tot_rec += rec_loss.item() / len(train_loader)\n",
        "      tot_kl += kl.item() / len(train_loader)\n",
        "      # YOUR CODE HERE\n",
        "      \n",
        "\n",
        "\n",
        "    kl_hist.append(tot_kl)\n",
        "    rec_hist.append(tot_rec)\n",
        "    fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
        "    ax[0].plot(kl_hist)\n",
        "    ax[0].set_title('kl')\n",
        "    ax[0].set_yscale('log')\n",
        "    ax[1].plot(rec_hist)\n",
        "    ax[1].set_title('rec')\n",
        "    ax[1].set_yscale('log')\n",
        "    plt.pause(0.3);\n",
        "    clear_output(wait=True);\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA24U-ju5_Lo",
        "outputId": "e0b3c5ba-69f0-4eeb-91ed-2d6e85f6785c"
      },
      "source": [
        "device = 'cuda'\n",
        "vae = VAE(32)\n",
        "vae.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (encoder): Sequential(\n",
              "    (encode1): Sequential(\n",
              "      (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "      (1): ELU(alpha=1.0)\n",
              "      (2): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (elr): Linear(in_features=2304, out_features=32, bias=True)\n",
              "  )\n",
              "  (mu): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (logvar): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (decoder): Sequential(\n",
              "    (dlr): Linear(in_features=32, out_features=2304, bias=True)\n",
              "    (unflatten): Unflatten(dim=1, unflattened_size=(16, 12, 12))\n",
              "    (upsample): Upsample(size=(64, 64), mode=nearest)\n",
              "    (decode3): Sequential(\n",
              "      (0): ConvTranspose2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): ELU(alpha=1.0)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fks9wyrEpas",
        "outputId": "874d6a7e-ade4-4899-b6d3-a70216e9391c"
      },
      "source": [
        "dilation=1\n",
        "H_in, kernel_size, stride, padding, output_padding = 66, 1, 1, 1, 0\n",
        "\n",
        "\n",
        "(H_in - 1)*stride - 2*padding + dilation*(kernel_size - 1) + output_padding\n",
        "\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "-C73Hjrs62Sc",
        "outputId": "99e4cc28-e0fd-4039-f5e6-de9a8bc7deb2"
      },
      "source": [
        "train_vae(train_dataloader, 20, vae)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-536a6ffb9748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-d466086980fc>\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(train_loader, max_epoch, vae)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtot_kl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mrec_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fne78EfcpQGQ"
      },
      "source": [
        "# Plot reconstructions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2iOTRBepQGR"
      },
      "source": [
        "---\n",
        "<a id='Task2'></a>\n",
        "**Task 2. [15 pts]: Evaluation**\n",
        "\n",
        "Calculate NLL on a validation set, contating only 3 dots. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ0z4CKZlm9C"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYos7lppQGR"
      },
      "source": [
        "---\n",
        "<a id='Task2'></a>\n",
        "**Task 3. [5 pts]: Generalization [1]**\n",
        "\n",
        "* Sample 25 images from the VAE and draw them on the 5 $\\times$ 5 grid. \n",
        "* What can you say abuout generalization ability of the model based the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_RTeerzlm9H"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sDescbwpQGS"
      },
      "source": [
        "`comment`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3w3g7FepQGS"
      },
      "source": [
        "--- \n",
        "<a id='Task4'></a>\n",
        "**Task 4. [10 pts]: Generalization [2]** \n",
        "\n",
        "* Define 2 new validation sets: containing only 2 dots (batches 6 and 7) and only 4 dots (batches 6-7) \n",
        "* Plot reconstruction of 10 random from both dataset\n",
        "* Compute NLL of your VAE on these datasets\n",
        "* What can you say abuout generalization ability of the model based the results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfUUDn_5lm9F"
      },
      "source": [
        "# NLL for 5 dots per image\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6L1slpalm9J"
      },
      "source": [
        "`comment`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ginyu4S9lm9M"
      },
      "source": [
        "---\n",
        "<a id='Bonus'></a>\n",
        "## Bonus task [10 pts]\n",
        "\n",
        "Assume that we want to quantify the generalization ability of the model. To do that, we need to accurately compute number of dots on all the generated images.\n",
        "\n",
        "1. Train neural network, which will classify images based on number of dots with high probability (>95%)\n",
        "2. Generate 10'000 images from you VAE\n",
        "3. Classify generated images and plot the histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUflqQyvlm9M"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}